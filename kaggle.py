# -*- coding: utf-8 -*-
"""kaggle.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TPFf2eibB8BTftckX8U5YdccHWdLM9VG
"""

#Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
import tensorflow as tf
import tensorflow_hub as hub

train = pd.read_csv('train.csv')
train

intents = pd.read_csv('intents.csv')
intents

user_queries = pd.read_csv('user_queries.csv')
user_queries

train_data = pd.merge(train, user_queries, on='query_id')
train_data

intents_list = intents.intent_id.tolist()
intents_list

for intent in intents_list:
  train_data.insert(len(train_data.columns), column=intent, value = [0]*len(train_data))
train_data

#needs to be rewritten!!
intent_ls = train_data['intents'].tolist()

for idx, it in enumerate(intent_ls):
    try:
        intent_item = it.split(" ")
    #print(intent_item)
        for i in intent_item:
            train_data.at[idx, i] = 1
    except:
        i = 'i'+ str(it)
        train_data.at[idx, i] = 1
train_data

X_train, X_test, y_train, y_test = train_test_split(train_data['query'], train_data[intents_list], test_size=0.1, random_state=666)

embedding = "https://tfhub.dev/google/nnlm-en-dim50/2"
hub_layer = hub.KerasLayer(embedding, input_shape=[], 
                           dtype=tf.string, trainable=True)
hub_layer(X_train[:3])

model = tf.keras.Sequential()
model.add(hub_layer)
model.add(tf.keras.layers.Dense(units=len(intents_list), activation='relu'))
#model.add(tf.keras.layers.Dense(len(intents_list)))

model.summary()

model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              optimizer='adam',
              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))

train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))
valid_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))
train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)
valid_dataset = valid_dataset.batch(16).prefetch(tf.data.AUTOTUNE)

history = model.fit(train_dataset, epochs=20, validation_data = valid_dataset)

test = pd.read_csv('test.csv')
test = pd.merge(test, user_queries, on=["query_id"])
test

predictions = model.predict(test['query'])

result = np.where(predictions > 0.1, 1 , 0)
result

#needs to be rewritten!!
intent_pred = []
for i in range(predictions.shape[0]):
    re = result[i]
    tmp = ""
    for idx, it in enumerate(re):
        if it == 1.0:
            tmp = tmp + " " + intents_list[idx]
    tmp = tmp.strip()
    intent_pred.append(tmp) 
intent_pred

test['intents'] = intent_pred

submit = test[['query_id', 'intents']]
#submit.to_csv('submit.csv', index = False)
submit

